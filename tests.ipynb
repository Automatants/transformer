{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2SdpaAttention(\n",
      "          (c_attn): Conv1D(nf=2304, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=768)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D(nf=3072, nx=768)\n",
      "          (c_proj): Conv1D(nf=768, nx=3072)\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer.wte.weight',\n",
       " 'transformer.wpe.weight',\n",
       " 'transformer.h.0.ln_1.weight',\n",
       " 'transformer.h.0.ln_1.bias',\n",
       " 'transformer.h.0.attn.c_attn.weight',\n",
       " 'transformer.h.0.attn.c_attn.bias',\n",
       " 'transformer.h.0.attn.c_proj.weight',\n",
       " 'transformer.h.0.attn.c_proj.bias',\n",
       " 'transformer.h.0.ln_2.weight',\n",
       " 'transformer.h.0.ln_2.bias',\n",
       " 'transformer.h.0.mlp.c_fc.weight',\n",
       " 'transformer.h.0.mlp.c_fc.bias',\n",
       " 'transformer.h.0.mlp.c_proj.weight',\n",
       " 'transformer.h.0.mlp.c_proj.bias',\n",
       " 'transformer.h.1.ln_1.weight',\n",
       " 'transformer.h.1.ln_1.bias',\n",
       " 'transformer.h.1.attn.c_attn.weight',\n",
       " 'transformer.h.1.attn.c_attn.bias',\n",
       " 'transformer.h.1.attn.c_proj.weight',\n",
       " 'transformer.h.1.attn.c_proj.bias',\n",
       " 'transformer.h.1.ln_2.weight',\n",
       " 'transformer.h.1.ln_2.bias',\n",
       " 'transformer.h.1.mlp.c_fc.weight',\n",
       " 'transformer.h.1.mlp.c_fc.bias',\n",
       " 'transformer.h.1.mlp.c_proj.weight',\n",
       " 'transformer.h.1.mlp.c_proj.bias',\n",
       " 'transformer.h.2.ln_1.weight',\n",
       " 'transformer.h.2.ln_1.bias',\n",
       " 'transformer.h.2.attn.c_attn.weight',\n",
       " 'transformer.h.2.attn.c_attn.bias',\n",
       " 'transformer.h.2.attn.c_proj.weight',\n",
       " 'transformer.h.2.attn.c_proj.bias',\n",
       " 'transformer.h.2.ln_2.weight',\n",
       " 'transformer.h.2.ln_2.bias',\n",
       " 'transformer.h.2.mlp.c_fc.weight',\n",
       " 'transformer.h.2.mlp.c_fc.bias',\n",
       " 'transformer.h.2.mlp.c_proj.weight',\n",
       " 'transformer.h.2.mlp.c_proj.bias',\n",
       " 'transformer.h.3.ln_1.weight',\n",
       " 'transformer.h.3.ln_1.bias',\n",
       " 'transformer.h.3.attn.c_attn.weight',\n",
       " 'transformer.h.3.attn.c_attn.bias',\n",
       " 'transformer.h.3.attn.c_proj.weight',\n",
       " 'transformer.h.3.attn.c_proj.bias',\n",
       " 'transformer.h.3.ln_2.weight',\n",
       " 'transformer.h.3.ln_2.bias',\n",
       " 'transformer.h.3.mlp.c_fc.weight',\n",
       " 'transformer.h.3.mlp.c_fc.bias',\n",
       " 'transformer.h.3.mlp.c_proj.weight',\n",
       " 'transformer.h.3.mlp.c_proj.bias',\n",
       " 'transformer.h.4.ln_1.weight',\n",
       " 'transformer.h.4.ln_1.bias',\n",
       " 'transformer.h.4.attn.c_attn.weight',\n",
       " 'transformer.h.4.attn.c_attn.bias',\n",
       " 'transformer.h.4.attn.c_proj.weight',\n",
       " 'transformer.h.4.attn.c_proj.bias',\n",
       " 'transformer.h.4.ln_2.weight',\n",
       " 'transformer.h.4.ln_2.bias',\n",
       " 'transformer.h.4.mlp.c_fc.weight',\n",
       " 'transformer.h.4.mlp.c_fc.bias',\n",
       " 'transformer.h.4.mlp.c_proj.weight',\n",
       " 'transformer.h.4.mlp.c_proj.bias',\n",
       " 'transformer.h.5.ln_1.weight',\n",
       " 'transformer.h.5.ln_1.bias',\n",
       " 'transformer.h.5.attn.c_attn.weight',\n",
       " 'transformer.h.5.attn.c_attn.bias',\n",
       " 'transformer.h.5.attn.c_proj.weight',\n",
       " 'transformer.h.5.attn.c_proj.bias',\n",
       " 'transformer.h.5.ln_2.weight',\n",
       " 'transformer.h.5.ln_2.bias',\n",
       " 'transformer.h.5.mlp.c_fc.weight',\n",
       " 'transformer.h.5.mlp.c_fc.bias',\n",
       " 'transformer.h.5.mlp.c_proj.weight',\n",
       " 'transformer.h.5.mlp.c_proj.bias',\n",
       " 'transformer.h.6.ln_1.weight',\n",
       " 'transformer.h.6.ln_1.bias',\n",
       " 'transformer.h.6.attn.c_attn.weight',\n",
       " 'transformer.h.6.attn.c_attn.bias',\n",
       " 'transformer.h.6.attn.c_proj.weight',\n",
       " 'transformer.h.6.attn.c_proj.bias',\n",
       " 'transformer.h.6.ln_2.weight',\n",
       " 'transformer.h.6.ln_2.bias',\n",
       " 'transformer.h.6.mlp.c_fc.weight',\n",
       " 'transformer.h.6.mlp.c_fc.bias',\n",
       " 'transformer.h.6.mlp.c_proj.weight',\n",
       " 'transformer.h.6.mlp.c_proj.bias',\n",
       " 'transformer.h.7.ln_1.weight',\n",
       " 'transformer.h.7.ln_1.bias',\n",
       " 'transformer.h.7.attn.c_attn.weight',\n",
       " 'transformer.h.7.attn.c_attn.bias',\n",
       " 'transformer.h.7.attn.c_proj.weight',\n",
       " 'transformer.h.7.attn.c_proj.bias',\n",
       " 'transformer.h.7.ln_2.weight',\n",
       " 'transformer.h.7.ln_2.bias',\n",
       " 'transformer.h.7.mlp.c_fc.weight',\n",
       " 'transformer.h.7.mlp.c_fc.bias',\n",
       " 'transformer.h.7.mlp.c_proj.weight',\n",
       " 'transformer.h.7.mlp.c_proj.bias',\n",
       " 'transformer.h.8.ln_1.weight',\n",
       " 'transformer.h.8.ln_1.bias',\n",
       " 'transformer.h.8.attn.c_attn.weight',\n",
       " 'transformer.h.8.attn.c_attn.bias',\n",
       " 'transformer.h.8.attn.c_proj.weight',\n",
       " 'transformer.h.8.attn.c_proj.bias',\n",
       " 'transformer.h.8.ln_2.weight',\n",
       " 'transformer.h.8.ln_2.bias',\n",
       " 'transformer.h.8.mlp.c_fc.weight',\n",
       " 'transformer.h.8.mlp.c_fc.bias',\n",
       " 'transformer.h.8.mlp.c_proj.weight',\n",
       " 'transformer.h.8.mlp.c_proj.bias',\n",
       " 'transformer.h.9.ln_1.weight',\n",
       " 'transformer.h.9.ln_1.bias',\n",
       " 'transformer.h.9.attn.c_attn.weight',\n",
       " 'transformer.h.9.attn.c_attn.bias',\n",
       " 'transformer.h.9.attn.c_proj.weight',\n",
       " 'transformer.h.9.attn.c_proj.bias',\n",
       " 'transformer.h.9.ln_2.weight',\n",
       " 'transformer.h.9.ln_2.bias',\n",
       " 'transformer.h.9.mlp.c_fc.weight',\n",
       " 'transformer.h.9.mlp.c_fc.bias',\n",
       " 'transformer.h.9.mlp.c_proj.weight',\n",
       " 'transformer.h.9.mlp.c_proj.bias',\n",
       " 'transformer.h.10.ln_1.weight',\n",
       " 'transformer.h.10.ln_1.bias',\n",
       " 'transformer.h.10.attn.c_attn.weight',\n",
       " 'transformer.h.10.attn.c_attn.bias',\n",
       " 'transformer.h.10.attn.c_proj.weight',\n",
       " 'transformer.h.10.attn.c_proj.bias',\n",
       " 'transformer.h.10.ln_2.weight',\n",
       " 'transformer.h.10.ln_2.bias',\n",
       " 'transformer.h.10.mlp.c_fc.weight',\n",
       " 'transformer.h.10.mlp.c_fc.bias',\n",
       " 'transformer.h.10.mlp.c_proj.weight',\n",
       " 'transformer.h.10.mlp.c_proj.bias',\n",
       " 'transformer.h.11.ln_1.weight',\n",
       " 'transformer.h.11.ln_1.bias',\n",
       " 'transformer.h.11.attn.c_attn.weight',\n",
       " 'transformer.h.11.attn.c_attn.bias',\n",
       " 'transformer.h.11.attn.c_proj.weight',\n",
       " 'transformer.h.11.attn.c_proj.bias',\n",
       " 'transformer.h.11.ln_2.weight',\n",
       " 'transformer.h.11.ln_2.bias',\n",
       " 'transformer.h.11.mlp.c_fc.weight',\n",
       " 'transformer.h.11.mlp.c_fc.bias',\n",
       " 'transformer.h.11.mlp.c_proj.weight',\n",
       " 'transformer.h.11.mlp.c_proj.bias',\n",
       " 'transformer.ln_f.weight',\n",
       " 'transformer.ln_f.bias',\n",
       " 'lm_head.weight']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.state_dict().keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm GPT-2, and my name is Lutz. I came to Rokkot from Hungary, I have a son\"},\n",
       " {'generated_text': \"Hello, I'm GPT-2, but the second episode, which is quite an interesting one, I've also written up to go with that\"},\n",
       " {'generated_text': \"Hello, I'm GPT-2, and I'm just a normal guy. I am a little bit like a bachelorette.\\n\"},\n",
       " {'generated_text': \"Hello, I'm GPT-2, so thank you so much!\\n\\nThe guys here at Team GPs are the best. We take\"},\n",
       " {'generated_text': \"Hello, I'm GPT-2, so I'm no one really worried about it. It's just not relevant as much as I'd like\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "set_seed(42)\n",
    "\n",
    "generator(\"Hello, I'm GPT-2,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class GPT2Config:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50257\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](attn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 16]) torch.Size([2, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "B, T, C = 2, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "head_size = 16\n",
    "key_getter = nn.Linear(C, head_size, bias=False)\n",
    "query_getter = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key_getter(x) # what does this token represent?\n",
    "q = query_getter(x) # what is this token looking for?\n",
    "\n",
    "print(k.shape, q.shape) # (B, T, head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "# (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "masked_attn_scores = q @ k.transpose(-2, -1)\n",
    "print(masked_attn_scores.shape) # (B, T, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3352,  0.2766, -0.3269,  0.4146, -0.3710, -0.1537, -0.0427,\n",
      "          -0.1284],\n",
      "         [ 0.2032,  0.0540, -0.1558,  0.3877, -0.2162,  1.1867,  0.3357,\n",
      "           0.4529],\n",
      "         [ 0.1822, -0.0837, -0.1061,  0.0885, -0.1683, -0.2100,  0.3359,\n",
      "          -0.1270],\n",
      "         [ 0.0239,  0.2039, -0.4211,  0.0917, -0.2047,  0.7772, -0.2167,\n",
      "           0.2692],\n",
      "         [-0.0526, -0.1334, -0.2102, -0.0587, -0.0825, -0.0722,  0.0053,\n",
      "           0.0309],\n",
      "         [-0.6124, -0.1651, -0.4587,  0.0884, -0.3484,  1.1364, -0.1815,\n",
      "           0.3092],\n",
      "         [-0.2513, -0.1293, -0.2516, -0.0489,  0.0567,  0.6245, -0.2788,\n",
      "           0.5562],\n",
      "         [ 0.0560,  0.2676,  0.1502, -0.0554,  0.2297, -0.0465,  0.1581,\n",
      "          -0.1356]],\n",
      "\n",
      "        [[-0.3393,  0.2480,  0.1879,  0.0455, -0.0734,  0.1622, -0.1689,\n",
      "           0.3859],\n",
      "         [ 0.0393,  0.2520, -0.4839, -0.3853,  0.1411,  0.0875, -0.0131,\n",
      "           0.0977],\n",
      "         [-0.1508, -0.1411,  0.2021,  0.1708, -0.2341, -0.0830, -0.0594,\n",
      "          -0.1101],\n",
      "         [-0.1811,  0.1074, -0.1218,  0.0140,  0.0144, -0.1160, -0.2158,\n",
      "          -0.0674],\n",
      "         [ 0.3363, -0.0828,  0.2928, -0.1615,  0.2250,  0.0688,  0.3847,\n",
      "          -0.1646],\n",
      "         [-0.0903,  0.2815, -0.2179,  0.0665,  0.2329,  0.1044,  0.1461,\n",
      "          -0.0153],\n",
      "         [ 0.1041, -0.0789, -0.3316,  0.2870, -0.3906, -0.0711, -0.2974,\n",
      "          -0.1424],\n",
      "         [-0.0756,  0.3248, -0.4631,  0.0739, -0.2128,  0.4943,  0.2945,\n",
      "          -0.1879]]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_attn_scores *= 1 / math.sqrt(head_size)\n",
    "print(masked_attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "trill = torch.tril(torch.ones(T, T))\n",
    "print(trill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3352,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "             -inf],\n",
      "         [ 0.2032,  0.0540,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "             -inf],\n",
      "         [ 0.1822, -0.0837, -0.1061,    -inf,    -inf,    -inf,    -inf,\n",
      "             -inf],\n",
      "         [ 0.0239,  0.2039, -0.4211,  0.0917,    -inf,    -inf,    -inf,\n",
      "             -inf],\n",
      "         [-0.0526, -0.1334, -0.2102, -0.0587, -0.0825,    -inf,    -inf,\n",
      "             -inf],\n",
      "         [-0.6124, -0.1651, -0.4587,  0.0884, -0.3484,  1.1364,    -inf,\n",
      "             -inf],\n",
      "         [-0.2513, -0.1293, -0.2516, -0.0489,  0.0567,  0.6245, -0.2788,\n",
      "             -inf],\n",
      "         [ 0.0560,  0.2676,  0.1502, -0.0554,  0.2297, -0.0465,  0.1581,\n",
      "          -0.1356]],\n",
      "\n",
      "        [[-0.3393,    -inf,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "             -inf],\n",
      "         [ 0.0393,  0.2520,    -inf,    -inf,    -inf,    -inf,    -inf,\n",
      "             -inf],\n",
      "         [-0.1508, -0.1411,  0.2021,    -inf,    -inf,    -inf,    -inf,\n",
      "             -inf],\n",
      "         [-0.1811,  0.1074, -0.1218,  0.0140,    -inf,    -inf,    -inf,\n",
      "             -inf],\n",
      "         [ 0.3363, -0.0828,  0.2928, -0.1615,  0.2250,    -inf,    -inf,\n",
      "             -inf],\n",
      "         [-0.0903,  0.2815, -0.2179,  0.0665,  0.2329,  0.1044,    -inf,\n",
      "             -inf],\n",
      "         [ 0.1041, -0.0789, -0.3316,  0.2870, -0.3906, -0.0711, -0.2974,\n",
      "             -inf],\n",
      "         [-0.0756,  0.3248, -0.4631,  0.0739, -0.2128,  0.4943,  0.2945,\n",
      "          -0.1879]]], grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_attn_scores = masked_attn_scores.masked_fill(trill == 0, float('-inf'))\n",
    "print(masked_attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.5372, 0.4628, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.3974, 0.3046, 0.2979, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2559, 0.3063, 0.1640, 0.2738, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2109, 0.1946, 0.1802, 0.2096, 0.2047, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0782, 0.1222, 0.0911, 0.1575, 0.1018, 0.4492, 0.0000, 0.0000],\n",
      "         [0.1100, 0.1243, 0.1100, 0.1347, 0.1497, 0.2642, 0.1070, 0.0000],\n",
      "         [0.1211, 0.1497, 0.1331, 0.1084, 0.1441, 0.1093, 0.1342, 0.1000]],\n",
      "\n",
      "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.4470, 0.5530, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2913, 0.2941, 0.4146, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2169, 0.2894, 0.2301, 0.2636, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.2429, 0.1597, 0.2325, 0.1476, 0.2173, 0.0000, 0.0000, 0.0000],\n",
      "         [0.1409, 0.2044, 0.1240, 0.1648, 0.1947, 0.1712, 0.0000, 0.0000],\n",
      "         [0.1724, 0.1436, 0.1116, 0.2071, 0.1052, 0.1447, 0.1154, 0.0000],\n",
      "         [0.1074, 0.1602, 0.0729, 0.1247, 0.0936, 0.1898, 0.1555, 0.0960]]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_scores = F.softmax(masked_attn_scores, dim=-1)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "value_getter = nn.Linear(C, head_size, bias=False)\n",
    "v = value_getter(x)\n",
    "print(v.shape) # (B, T, head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "out = attn_scores @ v\n",
    "print(out.shape) # (B, T, head_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing our GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_correction import GPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained weights for model gpt2\n"
     ]
    }
   ],
   "source": [
    "gpt = GPT2.from_pretrained()\n",
    "gpt.eval()\n",
    "gpt.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GPT2LMHeadModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hf_gpt \u001b[38;5;241m=\u001b[39m \u001b[43mGPT2LMHeadModel\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m hf_gpt\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      3\u001b[0m hf_gpt\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GPT2LMHeadModel' is not defined"
     ]
    }
   ],
   "source": [
    "hf_gpt = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "hf_gpt.eval()\n",
    "hf_gpt.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_or_ours = \"ours\" # \"ours\" or \"hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sequences = 5\n",
    "max_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = tokenizer.encode(\"Hello, I'm GPT-2,\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).repeat(num_sequences, 1)\n",
    "x = tokens.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "torch.mps.manual_seed(42)\n",
    "\n",
    "while x.size(1) < max_length:\n",
    "    with torch.no_grad():\n",
    "        logits = gpt(x) if hf_or_ours == \"ours\" else hf_gpt(x)[0]\n",
    "        logits = logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        # topk sampling 50 by default on HF\n",
    "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
    "        ix = torch.multinomial(topk_probs, num_samples=1)\n",
    "        # gather the tokens at the specific indices\n",
    "        xcol = torch.gather(topk_indices, dim=-1, index=ix)\n",
    "        x = torch.cat((x, xcol), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Hello, I'm GPT-2, , , , , , , , , , , , , , , , , , , , , ,\n",
      "> Hello, I'm GPT-2,,,,,,,,,,,,,,,,,,,,,,\n",
      "> Hello, I'm GPT-2,,,,,,,,,,,,,,,,,,\n",
      "\n",
      "\"}\"\n",
      "> Hello, I'm GPT-2,,,,,,,,,,,,,,,,,,,,,,\n",
      "> Hello, I'm GPT-2,,,,,,,,,,\n",
      "\n",
      "5 5 0 0 0 0 0 0 0 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_sequences):\n",
    "    tokens = x[i, :max_length].tolist()\n",
    "    decoded = tokenizer.decode(tokens)\n",
    "    print(\">\", decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
